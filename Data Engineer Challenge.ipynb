{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fcbf81",
   "metadata": {},
   "source": [
    "## Technical Questions\n",
    "\n",
    "1. Data Modeling and Warehousing\n",
    "Question: How do you design a schema for a new data-intensive application?\n",
    "    1. First of all it is important to identify where is data comming from and on which format type\n",
    "    2. Then a check up on the data is needed to understand it and get as much familiar as possible to it\n",
    "    3. once checked, it is important to visualize if theres duplicated data/relations that can be normalized as mcuh as possible (depending on project nature/requirements)\n",
    "    4. At this point after normalization, Fact table(s) and Dimension table(s) are now identified\n",
    "    5. It is now time to generate an Entity-Relationship Diagram (ERD) with proper one-one or one-many relations\n",
    "    6. Once ERD is completed, Schema can be generation based on the ERD. Paying attention on column data types and Keys\n",
    "    7. When Schema is finalized it is now time to ingest/dump data into tables (tables population)\n",
    "\n",
    "    NOTE: Make sure the source data is suitable to comply with schema requirements (data types, Nulls, etc). Also a good practice\n",
    "    is to clean data as much as possible (removing unexpected characters, blank spaces, duplicated values, etc)\n",
    "\n",
    "2. ETL/ELT Design and Implementation\n",
    "Question: Describe a complex ETL pipeline you've designed. What were some of the challenges, and how did you address them?\n",
    "    \n",
    "    Electric Transmission Diagrams Migration Project:\n",
    "    \n",
    "    The requirement was to gather all required information from source data (provided by customer), then use all that extracted information and transform it to comply with some\n",
    "    previously defined conversion rules. This conversion rules had to be designed due to the migration was being done between systems of different suppliers. Once data were transformed\n",
    "    it was used to generate XML files, that will later be loaded into the new system which consumed those XML to populate its operational database.\n",
    "    All this process was divided on different stages as follows:\n",
    "\n",
    "    1. Identify data sources: Two data sources were identified, a set of 8k+ SVG files (Containing electric diagram data) and a MySQL database (containing electric network SCADA model data)\n",
    "    2. Old vs New system mapping: In order to identify common functionalities/atributes between systems and differences.\n",
    "    3. Conversion rules: from the mapping, the differences between systems were spotted, having to generate migration rules to transform/modify the differences to make them fit on the new system\n",
    "    4. Once having all needed data and insights to generate a correct migration process, a Python ELT tool was developed with the following stages:\n",
    "        - Extract data: This stage looped over all the 8k+ SVG files, parsed them and extracted all attribute,values from every tag, generating a list containing a dictionary per object, which\n",
    "        were later dumped into a CSV file\n",
    "        - Load data: On this stage, all the CSV files generated on previous stage were dumped into a MySQL DB in order to generate a warehouse on premises\n",
    "        - Transform data: This stage used the data on the warehouse and transformed it based on the conversion rules. Also this transformation phase\n",
    "        considered the SCADA model database, transforming the combination of this two sources in order to generate suitable data for the new system that was stored in new tables.\n",
    "        The data in this new tables was used to generate an XML version of the source SVG files, which will later be used to populate the new system operational data base.\n",
    "\n",
    "    NOTE: The procedure documentation can be found on my GitHub: https://github.com/SaurioAG/Electric_Transmission_Diagrams_Migration Code is not shared\n",
    "    due to No Disclosure Agreement (NDA) with customer.\n",
    "\n",
    "3. Data Infrastructure and Orchestration\n",
    "Question: What experience do you have with managing data infrastructure on-premises or in the cloud?\n",
    "\n",
    "    Most of my experience and projects have been on-premises enviroments.\n",
    "    Databases and systems allocated on company servers, which were deployed by system integrators.\n",
    "    I personaly do not have experience deploying physical infrastructure, just managing/using resources of an already deployed enviroment.\n",
    "    Deploying and using ETLs, populating  relational data bases and querying data from DBs.\n",
    "    Unfortunately I haven't had commercial experience with cloud enviroments.\n",
    "    However I'm willing and open to learn about cloud services, AWS and GCP are on my scope.\n",
    "\n",
    "4. Programming and Software Engineering Practices\n",
    "Question: What programming languages are you most comfortable with, and what libraries do you frequently use in data engineering projects?\n",
    "\n",
    "    Most of my experience at programming is with Python 3.10. Also I have used a variety of libraries across all the different projects I've worked on like:\n",
    "    - For Data Wrangling/Analysis: pandas, numpy, PySpark, re\n",
    "    - For Data Orchestration: airflow\n",
    "    - For DB connections: sqlite3, psycopg2, mysql\n",
    "    - For OS interaction: os, sys, glob\n",
    "    - For Web scraping: requests\n",
    "    - For HTML/XML/SVG parsing: bs4, xml\n",
    "\n",
    "    Also I've proficiency queying with SQL\n",
    "\n",
    "5. Data Security and Compliance\n",
    "Question: How do you implement security measures in your data engineering projects?\n",
    "\n",
    "    On DB side:\n",
    "        Encrypting table sensitive information.\n",
    "        Managing user privileges/authorities/access\n",
    "    On Code:\n",
    "        Avoid harcoding information that can compromise servers/data\n",
    "        Use enviroment variables to retrieve data like users, passwords, mac addresses, ip addresses\n",
    "    On server OS:\n",
    "        User Authentication\n",
    "        Audit Logging and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bed726",
   "metadata": {},
   "source": [
    "Please explain how you ussually follow CI/CD pipelines\n",
    "\n",
    "    Some of the practices I consider to preserve a good CI/CD pipelines health are:\n",
    "    1. Using version control: To have an historic of bug fixes, enhancements, new features, customer change requirements, etc.\n",
    "    2. Code Testing: wether unit testing, integrtation testing, smoke testing, end to end testing.\n",
    "    3. Iterative improvement: Code is never finished, as it can be continuously improved due to previous projects lessons learned, customer feedback, customer change requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967bcf82",
   "metadata": {},
   "source": [
    "## Code Chanllenge SQL\n",
    "\n",
    "Code Challenge Description\n",
    "Title: Building a MySQL Database Interface in Python\n",
    "\n",
    "Objective:\n",
    "You are tasked with creating a Python application that interfaces with a MySQL database. The application will manage a dataset representing sales data for a tech company that sells various products across multiple countries. Your goal is to establish a database connection, create a table, and populate this table with sample data.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Create a Database Connection:\n",
    "Implement a Python function to establish a connection to a MySQL database using provided credentials (host, username, password, and database name).\n",
    "Define and Create a Table:\n",
    "Write SQL commands within your Python script to create a table named sales. This table should have columns for id, country, category, price, quantity, and final_sales, with appropriate data types.\n",
    "Insert Data:\n",
    "Prepare a series of SQL INSERT statements to populate the sales table with the provided sample data. Ensure each record accurately reflects the sales data format.\n",
    "Execute Queries:\n",
    "Write functions to execute SQL queries to create the table and insert data into the table. Include error handling to manage potential SQL execution errors.\n",
    "Expected Deliverables:\n",
    "\n",
    "A Python script that can be run to connect to a MySQL database, create the necessary table, and populate it with data.\n",
    "Your script should handle common errors that might occur during database operations, such as connection failures or SQL syntax errors.\n",
    "Evaluation Criteria:\n",
    "\n",
    "Correctness: The script should correctly execute all database operations without errors.\n",
    "\n",
    "Code Quality: Code should be clear, well-organized, and appropriately commented.\n",
    "\n",
    "Error Handling: The script should effectively handle and report errors during database operations.\n",
    "\n",
    "Efficiency: SQL operations should be written efficiently to optimize execution.\n",
    "\n",
    "Setup Instructions\n",
    "Just use mysql local community server and a made up data set related to sales of devices in a tech company \n",
    "https://dev.mysql.com/downloads/mysql/\n",
    "\n",
    "Table 1 Sales \n",
    "    product_id ,\n",
    "    country ,\n",
    "    category ,\n",
    "    price ,\n",
    "    quantity ,\n",
    "    final_sales\n",
    "    \n",
    "Table 2 Product\n",
    "    id  PRIMARY KEY,\n",
    "    category ,\n",
    "    capacity ,\n",
    "    color ,\n",
    "    screen_size ,\n",
    "    memory ,\n",
    "    other_specs .\n",
    "Ensure you have mysql-connector-python installed in your environment. If not, you can install it using pip install mysql-connector-python.\n",
    "\n",
    "Tips for Success\n",
    "Test each part of your script incrementally to ensure that each function behaves as expected.\n",
    "Consider the edge cases, such as what happens if the table already exists or the database connection cannot be established.\n",
    "\n",
    "This challenge is designed to test your ability to integrate Python programming with SQL database management, reflecting tasks you may handle as a data engineer in our organization. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7bcd46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in c:\\users\\saurio\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (8.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8118b",
   "metadata": {},
   "source": [
    "## Create a MySQL Database using Python\n",
    "### First, you need to connect to your MySQL server and create a new database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8c28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector as conn\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def db_conn(host_id: str, user_id: str, psw: str):\n",
    "    \"\"\"\n",
    "    Handles the connection to a MySQL server to \"sales\" Data Base\n",
    "    Receives DB connection credentials in order to connect to DB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = conn.connect(\n",
    "            host = host_id,\n",
    "            # database = db_name,\n",
    "            user = user_id,\n",
    "            password = psw)\n",
    "    except conn.errors.DatabaseError as error:\n",
    "        if \"2003\" in  str(error):\n",
    "            return print(f'An exception has occurred: {error}. Server is not reachable')\n",
    "        elif \"1045\" in str(error):\n",
    "            return print(f'An exception has occurred: {error}. Any of the provided credentials is incorrect')\n",
    "        elif \"1049\" in str(error):\n",
    "            \"\"\"\n",
    "            Unknown database \n",
    "            \"\"\"\n",
    "            pass\n",
    "        else:\n",
    "            return print(f'An exception has occurred: {error}.')\n",
    "    return connection\n",
    "\n",
    "def db_cursor(db_connection):\n",
    "    \"\"\"\n",
    "    Generates a cursos object in order to handle queries to \"sales\" Data Base\n",
    "    \"\"\"\n",
    "    cursor = db_connection.cursor()\n",
    "    return cursor\n",
    "\n",
    "def create_database(db_connection, cursor, db_name: str):\n",
    "    \"\"\"\n",
    "    Creates sales DB\n",
    "    \"\"\"\n",
    "    query = f'CREATE DATABASE IF NOT EXISTS {db_name}\\\n",
    "        CHARACTER SET=utf8mb4\\\n",
    "        COLLATE=utf8mb4_bin\\\n",
    "        ENCRYPTION=\"N\"'\n",
    "    cursor.execute(query)\n",
    "    print(f'Creating Data Base')\n",
    "    db_connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13997158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Data Base\n"
     ]
    }
   ],
   "source": [
    "host_id = input(f'Provide host: ')\n",
    "db_name = input(f'To which DB you want to connect? ')\n",
    "user_id= input(f'User: ')\n",
    "psw = input(f'Pass: ')\n",
    "\n",
    "connection = db_conn(host_id, user_id, psw)\n",
    "if connection is not None:\n",
    "    cursor = db_cursor(connection)\n",
    "    create_database(connection, cursor, db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84329b",
   "metadata": {},
   "source": [
    "## Insert Data into the sales Table\n",
    "### The following Python script shows how to insert data into the sales table. We'll be adding rows using a batch insert for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caea8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sales_table(db_connection, cursor, db_name: str):\n",
    "    \"\"\"\n",
    "    Creates a table \"sales\" on the  Data base\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f'CREATE TABLE {db_name}.sales (\\\n",
    "            product_id int NOT NULL ,\\\n",
    "            country varchar(256) NOT NULL,\\\n",
    "            category varchar(128) NOT NULL,\\\n",
    "            price double NOT NULL,\\\n",
    "            quantity int NOT NULL,\\\n",
    "            final_sales double NOT NULL\\\n",
    "        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin'\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        print(f'Creating sales table')\n",
    "        db_connection.commit()\n",
    "    except conn.errors.ProgrammingError as error:\n",
    "        print(f'An exception has ocurred: {error}')\n",
    "        response = input(f'Do you want to remove existing table and create a new one? (yes/no) ')\n",
    "        if response.lower() == \"yes\":\n",
    "            drop_query = f'DROP TABLE {db_name}.sales'\n",
    "            cursor.execute(drop_query)\n",
    "            db_connection.commit()\n",
    "            cursor.execute(query)\n",
    "            db_connection.commit()\n",
    "            return print(\"New table sales has been created\")\n",
    "        else:\n",
    "            return print(\"Keep exisitng sales table\")\n",
    "\n",
    "def generate_sales_data() -> list:\n",
    "    \"\"\"\n",
    "    A function that generates a list of tupples with random sales data and a list with unique product ids\n",
    "    \"\"\"\n",
    "    country = [\"Mexico\", \"Canada\", \"US\"]\n",
    "    category = [\"phone\", \"tablet\", \"laptop\"]\n",
    "    ids = [id for id in range(1, 11)]\n",
    "    list_of_rows = [(random.choice(ids), random.choice(country),\n",
    "                        random.choice(category), random.randint(3000, 30000),\n",
    "                        random.randint(1,10), random.randint(3000, 100000))\n",
    "                        for item in range(0, 100)]\n",
    "    id_list = list(set([row[0] for row in list_of_rows]))\n",
    "    print(id_list)\n",
    "    return list_of_rows, id_list\n",
    "\n",
    "def populate_sales_table(records: list, cursor, db_connection, db_name: str):\n",
    "    \"\"\"\n",
    "    Receives a list of tupples that contain each row data for the \"sales\" table.\n",
    "    Each tupple element should match the data type deffined on the table schema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_query = f'INSERT INTO {db_name}.sales (product_id, country, category, price, quantity, final_sales) VALUES'\n",
    "        for row in records:\n",
    "            values = f'({row[0]}, \"{row[1]}\", \"{row[2]}\", {row[3]}, {row[4]}, {row[5]}),'\n",
    "            if type(row[1]) != str or type(row[2]) != str:\n",
    "                return print(f'You are trying to use a number on a string field. row: {row}')\n",
    "            else:\n",
    "                full_query = f'{full_query} {values}'\n",
    "        cursor.execute(full_query[:len(full_query)-1])\n",
    "        print(f'Populating sales table')\n",
    "        db_connection.commit()\n",
    "    except conn.errors.ProgrammingError as error:\n",
    "        if \"1064\" in str(error):\n",
    "            return print(f'You are trying to fill a field with an empty value. Fields do not accept NULL values.')\n",
    "        elif \"1054\" in str(error):\n",
    "            return print(f'You are trying to fill a field with incorrect data type.')\n",
    "        else:\n",
    "            return print(f'An exception has ocurred: {error}')\n",
    "    except IndexError:\n",
    "        return print(f'The row you are trying to populate is missing a value. It should have 6 values on it.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0845f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception has ocurred: 1050 (42S01): Table 'sales' already exists\n",
      "New table sales has been created\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Populating sales table\n",
      "    product_id country category  price  quantity  final_sales\n",
      "0            7  Canada   tablet  17720         9        76393\n",
      "1            2  Canada   tablet   6610         4        58201\n",
      "2            4  Canada   tablet  26574         9        83470\n",
      "3            4  Mexico    phone  16263        10        99147\n",
      "4            5  Canada   laptop  24646         1        33662\n",
      "..         ...     ...      ...    ...       ...          ...\n",
      "95          10  Mexico    phone   9468         7        55110\n",
      "96           7      US   tablet  11149         2        99203\n",
      "97          10  Mexico    phone  10951         7        37471\n",
      "98           5  Mexico   laptop  27992         7        45883\n",
      "99           5  Mexico   tablet   6357         5        56419\n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "create_sales_table(connection, cursor, db_name)\n",
    "sales_data = generate_sales_data()\n",
    "populate_sales_table(sales_data[0], cursor = cursor, db_connection = connection, db_name = db_name)\n",
    "sales_df = pd.DataFrame(sales_data[0], columns = [\"product_id\", \"country\", \"category\", \"price\", \"quantity\", \"final_sales\"])\n",
    "print(sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b5b58",
   "metadata": {},
   "source": [
    "## Code to Create the product Table\n",
    "### First, here's the SQL command to create the product table with various specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099f9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_product_table(db_connection, cursor, db_name: str):\n",
    "    \"\"\"\n",
    "    Creates a table \"product\" on the  Data base\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f'CREATE TABLE {db_name}.product (\\\n",
    "            id int NOT NULL,\\\n",
    "            category varchar(128) NOT NULL,\\\n",
    "            capacity int NOT NULL,\\\n",
    "            color varchar(128) NOT NULL,\\\n",
    "            screen_size int NOT NULL,\\\n",
    "            memory int NOT NULL,\\\n",
    "            other_specs varchar(128) NOT NULL,\\\n",
    "            PRIMARY KEY (id)\\\n",
    "        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin'\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        print(f'Creating product table')\n",
    "        db_connection.commit()\n",
    "    except conn.errors.ProgrammingError as error:\n",
    "        print(f'An exception has ocurred: {error}')\n",
    "        response = input(f'Do you want to remove existing table and create a new one? (yes/no) ')\n",
    "        if response.lower() == \"yes\":\n",
    "            drop_query = f'DROP TABLE {db_name}.product'\n",
    "            cursor.execute(drop_query)\n",
    "            db_connection.commit()\n",
    "            cursor.execute(query)\n",
    "            db_connection.commit()\n",
    "            return print(\"New table product has been created\")\n",
    "        else:\n",
    "            return print(\"Keep exisitng product table\")\n",
    "\n",
    "def generate_product_data(sales_ids: list) -> list:\n",
    "    \"\"\"\n",
    "    A function that generates a list of tupples with random IT product data. Which receives a list of unique product ids from sales data.\n",
    "    Te received list ensures to have unique product ids (Keys) on the product table.\n",
    "    \"\"\"\n",
    "    list_of_rows = []\n",
    "    ids = sales_ids\n",
    "    category = [\"phone\", \"tablet\", \"laptop\"]\n",
    "    capacity = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "    color = [\"white\", \"black\", \"red\", \"blue\", \"pink\"]\n",
    "    size = [5, 6, 7, 8, 9, 10, 12, 15, 17]\n",
    "    specs = [\"fast_charge\", \"usb_c\", \"5G\"]\n",
    "    list_of_rows = [(ids.pop(0), random.choice(category),\n",
    "                     random.choice(capacity), random.choice(color),\n",
    "                     random.choice(size), random.choice(capacity),\n",
    "                     random.choice(specs)) for item in range(0, 10)]\n",
    "    return list_of_rows\n",
    "\n",
    "def populate_product_table(records: list, cursor, db_connection, db_name: str):\n",
    "    \"\"\"\n",
    "    Receives a list of tupples that contain each row data for the \"product\" table.\n",
    "    Each tupple element should match the data type deffined on the table schema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_query = f'INSERT INTO {db_name}.product (id, category, capacity, color, screen_size, memory, other_specs) VALUES'\n",
    "        for row in records:\n",
    "            values = f'({row[0]}, \"{row[1]}\", {row[2]}, \"{row[3]}\", {row[4]}, {row[5]}, \"{row[6]}\"),'\n",
    "            if type(row[1]) != str or type(row[3]) != str or type(row[6]) != str:\n",
    "                return print(f'You are trying to use a number on a string field. row: {row}')\n",
    "            else:\n",
    "                full_query = f'{full_query} {values}'\n",
    "        cursor.execute(full_query[:len(full_query)-1])\n",
    "        print(f'Populating product table')\n",
    "        db_connection.commit()\n",
    "    except conn.errors.ProgrammingError as error:\n",
    "        if \"1064\" in str(error):\n",
    "            return print(f'You are trying to fill a field with an empty value. Fields do not accept NULL values.')\n",
    "        elif \"1054\" in str(error):\n",
    "            print(error)\n",
    "            return print(f'You are trying to fill a field with incorrect data type.')\n",
    "        else:\n",
    "            return print(f'An exception has ocurred: {error}')\n",
    "    except IndexError:\n",
    "        return print(f'The row you are trying to populate is missing a value. It should have 7 values on it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17bdc36",
   "metadata": {},
   "source": [
    "## Python Code to Execute the Table Creation and Insert Data\n",
    "### Now, let's integrate this into your Python script to create the table and then populate it with some sample data(please just create dummy data ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec82935e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception has ocurred: 1050 (42S01): Table 'product' already exists\n",
      "New table product has been created\n",
      "Populating product table\n",
      "   id category  capacity  color  screen_size  memory  other_specs\n",
      "0   1    phone       512   blue            8       1  fast_charge\n",
      "1   2   laptop         1   pink            6       4  fast_charge\n",
      "2   3   tablet        16    red           15      16           5G\n",
      "3   4   tablet       128  white            7       1        usb_c\n",
      "4   5    phone         2   blue            7     256        usb_c\n",
      "5   6   tablet       256  white            7      32  fast_charge\n",
      "6   7   tablet       256  white           17       4           5G\n",
      "7   8    phone       512   pink           15      64           5G\n",
      "8   9    phone       128  black           17     256           5G\n",
      "9  10    phone         4  black            7      64  fast_charge\n"
     ]
    }
   ],
   "source": [
    "create_product_table(connection, cursor, db_name)\n",
    "product_data = generate_product_data(sales_data[1])\n",
    "populate_product_table(product_data, cursor = cursor, db_connection = connection, db_name = db_name)\n",
    "product_df = pd.DataFrame(product_data, columns = [\"id\", \"category\", \"capacity\", \"color\", \"screen_size\", \"memory\", \"other_specs\"])\n",
    "print(product_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e874fec",
   "metadata": {},
   "source": [
    "## Use Case: Detailed Sales Analysis\n",
    "### Objective:\n",
    "\n",
    "Determine the top-selling product categories in each country.\n",
    "Retrieve detailed product specifications for these top-selling products.\n",
    "Provide additional insights like the total number of distinct products sold and the maximum sales recorded for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59b8b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_sell_country_category(db_connection, cursor, db_name: str):\n",
    "    \"\"\"\n",
    "    Determines the top-selling product categories in each country.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f'CREATE TABLE {db_name}.top_sell_country_category AS (\\\n",
    "                    SELECT product_id, country, category, total\\\n",
    "                        FROM (\\\n",
    "                            SELECT product_id, country, category, sum(final_sales) AS total,\\\n",
    "                                ROW_NUMBER() OVER (PARTITION BY country ORDER BY SUM(final_sales) DESC) AS category_rank\\\n",
    "                                FROM {db_name}.sales\\\n",
    "                                GROUP BY country, category) AS rank_table\\\n",
    "                    WHERE category_rank = 1)'\n",
    "        cursor.execute(query)\n",
    "        print(f'Creating top_sell_country_category table')\n",
    "        db_connection.commit()\n",
    "        result = cursor.fetchall()\n",
    "        dataframe = pd.DataFrame(result, columns=[\"Product_id\", \"Country\", \"Category\", \"Total_sales\"])\n",
    "    except conn.errors.ProgrammingError as error:\n",
    "        print(f'An exception has ocurred: {error}')\n",
    "        response = input(f'Do you want to remove existing table and create a new one? (yes/no) ')\n",
    "        if response.lower() == \"yes\":\n",
    "            drop_query = f'DROP TABLE {db_name}.top_sell_country_category'\n",
    "            cursor.execute(drop_query)\n",
    "            db_connection.commit()\n",
    "            cursor.execute(query)\n",
    "            db_connection.commit()\n",
    "            cursor.execute(f'SELECT * FROM {db_name}.top_sell_country_category')\n",
    "            result = cursor.fetchall()\n",
    "            dataframe = pd.DataFrame(result, columns=[\"Product_id\", \"Country\", \"Category\", \"Total_sales\"])\n",
    "            print(\"New table top_sell_country_category has been created\")\n",
    "            return dataframe\n",
    "        else:\n",
    "            return print(\"Keep exisitng top_sell_country_category table\")\n",
    "\n",
    "def top_product_specs(cursor, db_name: str):\n",
    "    \"\"\"\n",
    "    Retrieves detailed product specifications for these top-selling products\n",
    "    \"\"\"\n",
    "    query = f'SELECT p.*, tscc.country, s.final_sales\\\n",
    "                FROM {db_name}.product p\\\n",
    "                JOIN\\\n",
    "                    {db_name}.top_sell_country_category tscc ON tscc.product_id = p.id\\\n",
    "                JOIN \\\n",
    "                    {db_name}.sales s ON tscc.product_id = s.product_id'\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    dataframe = pd.DataFrame(result, columns=[\"id\", \"Category\", \"Capacity\", \"Color\", \"Screen_size\", \"Memory\", \"Other_specs\", \"Country\", \"Final_sales\"])\n",
    "    return dataframe\n",
    "\n",
    "def total_distinct_products_sold(cursor, db_name: str):\n",
    "    \"\"\"\n",
    "    Determines the total number of distinct products sold\n",
    "    \"\"\"\n",
    "    query = f' SELECT \\\n",
    "\t                DISTINCT product_id,\\\n",
    "                    sum(quantity) \\\n",
    "                    FROM {db_name}.sales\\\n",
    "\t                GROUP BY product_id'\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    dataframe = pd.DataFrame(result, columns=[\"Product_id\", \"Total_sold\"])\n",
    "    return dataframe\n",
    "\n",
    "def max_sales_category(cursor, db_name: str):\n",
    "    \"\"\"\n",
    "    A query that determines the maximum sales recorded for each category\n",
    "    \"\"\"\n",
    "    query = f' SELECT \\\n",
    "\t                DISTINCT category,\\\n",
    "                    sum(final_sales) \\\n",
    "                    FROM {db_name}.sales\\\n",
    "\t                GROUP BY category'\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    dataframe = pd.DataFrame(result, columns=[\"Category\", \"Total_sales\"])\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f15a2fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-selling product categories in each country\n",
      "An exception has ocurred: 1050 (42S01): Table 'top_sell_country_category' already exists\n",
      "New table top_sell_country_category has been created\n",
      "   Product_id Country Category  Total_sales\n",
      "0           4  Canada    phone     629935.0\n",
      "1           1  Mexico   tablet     861812.0\n",
      "2          10      US   tablet     646149.0\n",
      "product specifications for the top-selling products\n",
      "    id Category  Capacity  Color  Screen_size  Memory  Other_specs Country  \\\n",
      "0    4   tablet       128  white            7       1        usb_c  Canada   \n",
      "1    4   tablet       128  white            7       1        usb_c  Canada   \n",
      "2   10    phone         4  black            7      64  fast_charge      US   \n",
      "3    4   tablet       128  white            7       1        usb_c  Canada   \n",
      "4   10    phone         4  black            7      64  fast_charge      US   \n",
      "5   10    phone         4  black            7      64  fast_charge      US   \n",
      "6   10    phone         4  black            7      64  fast_charge      US   \n",
      "7    4   tablet       128  white            7       1        usb_c  Canada   \n",
      "8   10    phone         4  black            7      64  fast_charge      US   \n",
      "9    1    phone       512   blue            8       1  fast_charge  Mexico   \n",
      "10   1    phone       512   blue            8       1  fast_charge  Mexico   \n",
      "11  10    phone         4  black            7      64  fast_charge      US   \n",
      "12   1    phone       512   blue            8       1  fast_charge  Mexico   \n",
      "13  10    phone         4  black            7      64  fast_charge      US   \n",
      "14   4   tablet       128  white            7       1        usb_c  Canada   \n",
      "15   1    phone       512   blue            8       1  fast_charge  Mexico   \n",
      "16   4   tablet       128  white            7       1        usb_c  Canada   \n",
      "17  10    phone         4  black            7      64  fast_charge      US   \n",
      "18   4   tablet       128  white            7       1        usb_c  Canada   \n",
      "19  10    phone         4  black            7      64  fast_charge      US   \n",
      "20   1    phone       512   blue            8       1  fast_charge  Mexico   \n",
      "21   4   tablet       128  white            7       1        usb_c  Canada   \n",
      "22  10    phone         4  black            7      64  fast_charge      US   \n",
      "23   4   tablet       128  white            7       1        usb_c  Canada   \n",
      "24  10    phone         4  black            7      64  fast_charge      US   \n",
      "25   1    phone       512   blue            8       1  fast_charge  Mexico   \n",
      "26   1    phone       512   blue            8       1  fast_charge  Mexico   \n",
      "27   1    phone       512   blue            8       1  fast_charge  Mexico   \n",
      "28   4   tablet       128  white            7       1        usb_c  Canada   \n",
      "29   1    phone       512   blue            8       1  fast_charge  Mexico   \n",
      "30  10    phone         4  black            7      64  fast_charge      US   \n",
      "31  10    phone         4  black            7      64  fast_charge      US   \n",
      "32  10    phone         4  black            7      64  fast_charge      US   \n",
      "33  10    phone         4  black            7      64  fast_charge      US   \n",
      "34  10    phone         4  black            7      64  fast_charge      US   \n",
      "\n",
      "    Final_sales  \n",
      "0       83470.0  \n",
      "1       99147.0  \n",
      "2       55452.0  \n",
      "3       56849.0  \n",
      "4       61829.0  \n",
      "5       28913.0  \n",
      "6        3042.0  \n",
      "7       67629.0  \n",
      "8       17619.0  \n",
      "9       43101.0  \n",
      "10      25014.0  \n",
      "11       5510.0  \n",
      "12      96830.0  \n",
      "13      53746.0  \n",
      "14      36861.0  \n",
      "15      71077.0  \n",
      "16      32180.0  \n",
      "17      96045.0  \n",
      "18      92401.0  \n",
      "19      24062.0  \n",
      "20      12494.0  \n",
      "21      52740.0  \n",
      "22      47690.0  \n",
      "23      26631.0  \n",
      "24      39055.0  \n",
      "25      67296.0  \n",
      "26      34607.0  \n",
      "27      50692.0  \n",
      "28      51907.0  \n",
      "29      50715.0  \n",
      "30      58872.0  \n",
      "31      84703.0  \n",
      "32      42125.0  \n",
      "33      55110.0  \n",
      "34      37471.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"top-selling product categories in each country\")\n",
    "print(top_sell_country_category(connection, cursor, db_name))\n",
    "\n",
    "print(\"product specifications for the top-selling products\")\n",
    "print(top_product_specs(cursor, db_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19536199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of distinct products sold\n",
      "   Product_id Total_sold\n",
      "0           7         94\n",
      "1           2         30\n",
      "2           4         55\n",
      "3           5         94\n",
      "4           8         60\n",
      "5          10         60\n",
      "6           3         28\n",
      "7           9         70\n",
      "8           6         52\n",
      "9           1         47\n",
      "maximum sales recorded for each category\n",
      "  Category  Total_sales\n",
      "0   tablet    1872685.0\n",
      "1    phone    1671575.0\n",
      "2   laptop    1369935.0\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of distinct products sold\")\n",
    "print(total_distinct_products_sold(cursor, db_name))\n",
    "\n",
    "print(\"maximum sales recorded for each category\")\n",
    "print(max_sales_category(cursor, db_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75266a00",
   "metadata": {},
   "source": [
    "## Code Chanllenge ETL on Python\n",
    "### ETL Code Challenge Description\n",
    "#### Title: ETL Process Simulation for Tech Company Sales Data\n",
    "\n",
    "Objective:\n",
    "Develop a Python-based ETL (Extract, Transform, Load) process that integrates data from multiple sources, applies specific transformations, and then loads the transformed data into a new table. This challenge tests your ability to handle data programmatically, showcasing your skills in data manipulation, SQL integration, and Python programming.\n",
    "\n",
    "Background:\n",
    "A tech company has multiple tables storing sales and product details. The sales table records transactions including the country, product category, and sales details. The product table includes specifications like capacity and color. Your task is to extract data from these tables, apply transformations to derive new insights, and load the results into a new structured format.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Extract:\n",
    "Write a Python function to retrieve data from the existing sales and product tables. The extracted data should include country, category, product capacity, color, quantity sold, and final sales amount.\n",
    "Transform:\n",
    "Implement transformations to calculate the total revenue for each product (defined as quantity * final_sales).\n",
    "Categorize each transaction based on sales volume into 'High', 'Medium', or 'Low'.\n",
    "Load:\n",
    "Design and create a new table called transformed_sales to store the transformed data.\n",
    "Load the transformed data into this table with appropriate field names and data types.\n",
    "Expected Deliverables:\n",
    "\n",
    "A Python script that implements the ETL process.\n",
    "The script should include functions for connecting to a MySQL database, executing SQL queries, and handling any potential errors.\n",
    "Documentation within the script explaining the purpose and functionality of each part of the code.\n",
    "Evaluation Criteria:\n",
    "\n",
    "Correctness: The script should correctly execute all steps of the ETL process without errors.\n",
    "Efficiency: Code and queries should be optimized for performance, especially when handling large datasets.\n",
    "Code Quality: The code should be well-organized, properly commented, and easy to read.\n",
    "Error Handling: The script should include robust error handling to manage and log potential issues during the database operations.\n",
    "Instructions for Execution:\n",
    "\n",
    "Just use mysql local community server and a made up data set related to sales of devices in a tech company and tables created on SQL portion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d98b1",
   "metadata": {},
   "source": [
    "Overview of the ETL Process\n",
    "Here's how we can structure the ETL process for your dataset:\n",
    "\n",
    "Extract: Retrieve data from the sales and product tables.\n",
    "Transform: Apply transformations to the data, such as computing additional metrics or modifying the format.\n",
    "Load: Load the transformed data into a new table or update the existing tables.\n",
    "1. Extract Data\n",
    "First, extract data from the MySQL database using the previously established connection and query functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f47189c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_db(cursor, db_name: str):\n",
    "    \"\"\"\n",
    "    Retrieve data from the existing sales and product tables. Including id, country, category, product capacity, color, quantity sold, and final sales amount\n",
    "    \"\"\"\n",
    "    query = f'SELECT\\\n",
    "                s.product_id,\\\n",
    "                s.country,\\\n",
    "                p.category,\\\n",
    "                p.capacity,\\\n",
    "                p.color,\\\n",
    "                s.quantity,\\\n",
    "                s.final_sales\\\n",
    "                FROM {db_name}.sales s\\\n",
    "                JOIN {db_name}.product p ON s.product_id = p.id'\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    dataframe = pd.DataFrame(result, columns = [\"id\", \"country\", \"category\", \"capacity\", \"color\", \"quantity\", \"final_sales\"])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c928106",
   "metadata": {},
   "source": [
    "2. Transform Data\n",
    "We will create a simple transformation function that, for example, calculates the total revenue per product and categorizes sales based on volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2db1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(extracted):\n",
    "    \"\"\"\n",
    "    This function Implement transformations to calculate the total revenue for each product (defined as quantity * final_sales).\n",
    "    Categorize each transaction based on sales volume into 'High', 'Medium', or 'Low'.\n",
    "    \"\"\"\n",
    "    dataframe = extracted\n",
    "    dataframe[\"total_revenue\"] = dataframe[\"quantity\"] * dataframe[\"final_sales\"]\n",
    "    # Determine some values to set a category scale\n",
    "    max_revenue = max(dataframe[\"total_revenue\"])\n",
    "    min_revenue = min(dataframe[\"total_revenue\"])\n",
    "    revenue_distance = max_revenue - min_revenue\n",
    "    half_revenue = revenue_distance/2\n",
    "    upper_limit = half_revenue*1.25\n",
    "    lower_limit = half_revenue*0.75\n",
    "    categories = []\n",
    "    for revenue in dataframe[\"total_revenue\"]:\n",
    "        if upper_limit < revenue <= max_revenue:\n",
    "            categories.append(\"High\")\n",
    "        elif upper_limit >= revenue >= lower_limit:\n",
    "            categories.append(\"Medium\")\n",
    "        elif lower_limit > revenue >= min_revenue:\n",
    "            categories.append(\"Low\")\n",
    "    dataframe[\"transact_category\"] = categories\n",
    "    list_of_rows = list(zip(*map(dataframe.get, dataframe)))\n",
    "    return dataframe, list_of_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b0279",
   "metadata": {},
   "source": [
    "3. Load Data\n",
    "Finally, write the transformed data back into a new table or an existing one. Here, let's assume we are creating a new table to store these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "221225d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformed_sales_table(db_connection, cursor, db_name: str):\n",
    "    \"\"\"\n",
    "    Creates a table \"transformed_sales\" on the  Data base\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f'CREATE TABLE {db_name}.transformed_sales (\\\n",
    "            id int NOT NULL,\\\n",
    "            country varchar(128) NOT NULL,\\\n",
    "            category varchar(128) NOT NULL,\\\n",
    "            capacity int NOT NULL,\\\n",
    "            color varchar(128) NOT NULL,\\\n",
    "            quantity int NOT NULL,\\\n",
    "            final_sales double NOT NULL,\\\n",
    "            total_revenue double NOT NULL,\\\n",
    "            transact_category varchar(128) NOT NULL\\\n",
    "        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin'\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        print(f'Creating transformed_sales table')\n",
    "        db_connection.commit()\n",
    "    except conn.errors.ProgrammingError as error:\n",
    "        print(f'An exception has ocurred: {error}')\n",
    "        response = input(f'Do you want to remove existing table and create a new one? (yes/no) ')\n",
    "        if response.lower() == \"yes\":\n",
    "            drop_query = f'DROP TABLE {db_name}.transformed_sales'\n",
    "            cursor.execute(drop_query)\n",
    "            db_connection.commit()\n",
    "            cursor.execute(query)\n",
    "            db_connection.commit()\n",
    "            return print(\"New table transformed_sales has been created\")\n",
    "        else:\n",
    "            return print(\"Keep exisitng transformed_sales table\")\n",
    "        \n",
    "def load_data(transformed_data: list, cursor, db_connection, db_name: str):\n",
    "    \"\"\"\n",
    "    Receives a list of tupples that contain each row data for the \"transformed_sales\" table.\n",
    "    Each tupple element should match the data type deffined on the table schema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_query = f'INSERT INTO {db_name}.transformed_sales (id, country, category, capacity, color, quantity, final_sales, total_revenue, transact_category) VALUES'\n",
    "        for row in transformed_data:\n",
    "            values = f'({row[0]}, \"{row[1]}\", \"{row[2]}\", {row[3]}, \"{row[4]}\", {row[5]}, {row[6]}, {row[7]}, \"{row[8]}\"),'\n",
    "            if type(row[1]) != str or type(row[2]) != str or type(row[4]) != str or type(row[8]) != str:\n",
    "                return print(f'You are trying to use a number on a string field. row: {row}')\n",
    "            else:\n",
    "                full_query = f'{full_query} {values}'\n",
    "        cursor.execute(full_query[:len(full_query)-1])\n",
    "        print(f'Populating product table')\n",
    "        db_connection.commit()\n",
    "    except conn.errors.ProgrammingError as error:\n",
    "        if \"1064\" in str(error):\n",
    "            return print(f'You are trying to fill a field with an empty value. Fields do not accept NULL values.')\n",
    "        elif \"1054\" in str(error):\n",
    "            print(error)\n",
    "            return print(f'You are trying to fill a field with incorrect data type.')\n",
    "        else:\n",
    "            return print(f'An exception has ocurred: {error}')\n",
    "    except IndexError:\n",
    "        return print(f'The row you are trying to populate is missing a value. It should have 9 values on it.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbc3af",
   "metadata": {},
   "source": [
    "Execution of ETL Process\n",
    "Now, combine these functions to perform the complete ETL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cba56b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception has ocurred: 1050 (42S01): Table 'transformed_sales' already exists\n",
      "New table transformed_sales has been created\n",
      "Populating product table\n",
      "Transformed sales data\n",
      "    id country category  capacity  color  quantity  final_sales  \\\n",
      "0    7  Canada   tablet       256  white         9      76393.0   \n",
      "1    2  Canada   laptop         1   pink         4      58201.0   \n",
      "2    4  Canada   tablet       128  white         9      83470.0   \n",
      "3    4  Mexico   tablet       128  white        10      99147.0   \n",
      "4    5  Canada    phone         2   blue         1      33662.0   \n",
      "..  ..     ...      ...       ...    ...       ...          ...   \n",
      "95  10  Mexico    phone         4  black         7      55110.0   \n",
      "96   7      US   tablet       256  white         2      99203.0   \n",
      "97  10  Mexico    phone         4  black         7      37471.0   \n",
      "98   5  Mexico    phone         2   blue         7      45883.0   \n",
      "99   5  Mexico    phone         2   blue         5      56419.0   \n",
      "\n",
      "    total_revenue transact_category  \n",
      "0        687537.0              High  \n",
      "1        232804.0               Low  \n",
      "2        751230.0              High  \n",
      "3        991470.0              High  \n",
      "4         33662.0               Low  \n",
      "..            ...               ...  \n",
      "95       385770.0            Medium  \n",
      "96       198406.0               Low  \n",
      "97       262297.0               Low  \n",
      "98       321181.0               Low  \n",
      "99       282095.0               Low  \n",
      "\n",
      "[100 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extracting data from DB\n",
    "extracted_data = extract_data_from_db(cursor, db_name)\n",
    "\n",
    "# Transofrming data\n",
    "transformed_data = transform_data(extracted_data)\n",
    "\n",
    "# Creating and populating transformed_sales table\n",
    "create_transformed_sales_table(connection, cursor, db_name)\n",
    "load_data(transformed_data[1], cursor = cursor, db_connection = connection, db_name = db_name)\n",
    "\n",
    "print(\"Transformed sales data\")\n",
    "print(transformed_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ac59e",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "This simulated ETL process in Python effectively demonstrates how to extract data from a relational database, apply meaningful transformations, and then load the processed data into a new storage system, providing practical hands-on experience with ETL concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf1e16",
   "metadata": {},
   "source": [
    "## Code Chanllenge Airflow\n",
    "\n",
    "Title: Developing an Airflow DAG for an Automated ETL Process\n",
    "\n",
    "Objective:\n",
    "The goal of this challenge is to develop a fully functional Airflow Directed Acyclic Graph (DAG) that orchestrates an ETL (Extract, Transform, Load) process. This process involves extracting data from a source, transforming this data, and loading it into a destination system.\n",
    "\n",
    "Background:\n",
    "Automating ETL tasks is crucial for ensuring data accuracy and availability in real-time or near-real-time for analysis and decision-making. Airflow is a platform used to programmatically author, schedule, and monitor workflows.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Complete the Python Functions:\n",
    "Extract Function: Implement logic to extract data from a predefined data source. This could be a database, a file, an API, or any simulated data source.\n",
    "Transform Function: Apply necessary data transformations which could include cleaning, aggregating, or any other form of data manipulation.\n",
    "Load Function: Implement the logic to load the transformed data into a specified target, which could be a database or a data warehouse.\n",
    "Integrate Functions with Airflow:\n",
    "Use the provided Airflow DAG skeleton to integrate your Python functions.\n",
    "Configure the DAG to ensure that tasks are executed in the correct order, handling dependencies correctly.\n",
    "Expected Deliverables:\n",
    "\n",
    "A fully implemented Airflow DAG named etl_process_dag with the specified tasks (extract, transform, load).\n",
    "Detailed documentation on:\n",
    "The data source and data format expected.\n",
    "The specific transformations applied.\n",
    "The destination system and data schema.\n",
    "Evaluation Criteria:\n",
    "\n",
    "Functionality: The DAG should execute without errors, and data should flow through the ETL process as intended.\n",
    "Code Quality: Code should be clean, well-commented, and follow best practices for Python and Airflow.\n",
    "Error Handling: Adequate error handling should be in place to manage common failures in data extraction, transformation, and loading.\n",
    "Scalability and Maintainability: The solution should be scalable and easy to maintain or modify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install airflow requests numpy sqlite3 bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16b339d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "# defining DAG arguments\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Roberto A.',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['dummy@email.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': True,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='etl_process_dag',\n",
    "    default_args=default_args,\n",
    "    description='Code Challenge Airflow',\n",
    "    schedule_interval=timedelta(days=1)\n",
    ")\n",
    "\n",
    "# defining tasks definitions\n",
    "\n",
    "def extract_bank_data():\n",
    "    \"\"\"\n",
    "    Goes into given url and web scrapes data from largest banks worldwide.\n",
    "    Extract just required data from a table and dumps into a CSV file\n",
    "    which is stored on the parent directory of this python file (opt/airflow)\n",
    "    \"\"\"\n",
    "    url = 'https://web.archive.org/web/20230908091635%20/https://en.wikipedia.org/wiki/List_of_largest_banks'\n",
    "    request = requests.get(url)\n",
    "    if request.status_code == 200:\n",
    "        soup = BeautifulSoup(request.text, features=\"html.parser\")\n",
    "        all_tables = soup.find_all('tbody')\n",
    "        DATA = {\n",
    "            'Name': [],\n",
    "            'MC_USD_Billion': []\n",
    "        }\n",
    "\n",
    "        for table in all_tables:\n",
    "            headers = table.find_all('th')\n",
    "            if 'Market cap' in headers[len(headers)-1]:\n",
    "                market_table = table\n",
    "                break\n",
    "\n",
    "        for row in market_table.find_all('tr')[1:]:\n",
    "            name_column = row.find_all('td')[1]\n",
    "            print(row.find_all('td')[2].contents[0][:-1])\n",
    "            DATA['Name'].append(name_column.find_all('a')[1].text.replace('\\n',''))\n",
    "            DATA['MC_USD_Billion'].append(row.find_all('td')[2].text.replace('\\n',''))\n",
    "        \n",
    "        dataframe = pd.DataFrame(DATA)\n",
    "        try:\n",
    "            dataframe.to_csv(\"List_of_largest_banks.csv\", index = False)\n",
    "        except FileExistsError:\n",
    "            os.remove(\"List_of_largest_banks.csv\")\n",
    "            dataframe.to_csv(\"List_of_largest_banks.csv\", index = False)\n",
    "    else:\n",
    "        print(\"The server is not reachable\")\n",
    "\n",
    "def extract_exchange_rate():\n",
    "    \"\"\"\n",
    "    Requests a csv file from a given url and extracs the data to generate a local CSV\n",
    "    which is which is stored on the parent directory of this python file (opt/airflow)\n",
    "    \"\"\"\n",
    "    url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-PY0221EN-Coursera/labs/v2/exchange_rate.csv'\n",
    "    csv = requests.get(url, allow_redirects=True)\n",
    "    with open('ex_rate_file.csv', 'wb') as file:\n",
    "        file.write(csv.content)\n",
    "\n",
    "def transform():\n",
    "    \"\"\"\n",
    "    Retrieves the data stored on the CSV files generated on previous extraction steps\n",
    "    transforms it and generates extra data. Then dumps it on a CSV file which is stored on the parent directory of this python file (opt/airflow)\n",
    "    \"\"\"\n",
    "    bank_data_df = pd.read_csv(\"List_of_largest_banks.csv\")\n",
    "    ex_rates_df = pd.read_csv(\"ex_rate_file.csv\")\n",
    "    bank_data_df['MC_USD_Billion'] = numpy.float64(bank_data_df['MC_USD_Billion'])\n",
    "    for rates in range(0, len(ex_rates_df)):\n",
    "        column_name = f'MC_{ex_rates_df[\"Currency\"].iloc[rates]}_Billion'\n",
    "        bank_data_df[column_name] = round(bank_data_df['MC_USD_Billion']*ex_rates_df['Rate'].iloc[rates],2)\n",
    "    try:\n",
    "        bank_data_df.to_csv(\"transformed_data.csv\", index = False)\n",
    "    except FileExistsError:\n",
    "        os.remove(\"transformed_data.csv\")\n",
    "        bank_data_df.to_csv(\"transformed_data.csv\", index = False)\n",
    "        \n",
    "def load_to_db():\n",
    "    \"\"\"\n",
    "    Generates a sqlite like db file, then gets data from the transformed_data CSV file and dumps it into DB file\n",
    "    called bank_data.db which is stored on the parent directory of this python file (opt/airflow)\n",
    "    \"\"\"\n",
    "    db_name = \"bank_data.db\"\n",
    "    try:\n",
    "        with open(db_name, \"x\"):\n",
    "            pass\n",
    "    except FileExistsError:\n",
    "        os.rmdir(db_name)\n",
    "        with open(db_name, \"x\"):\n",
    "            pass\n",
    "    transformed_data_df = pd.read_csv(\"transformed_data.csv\")\n",
    "    db_conn = sqlite3.connect(db_name)\n",
    "    transformed_data_df.to_sql(con = db_conn, name = \"transformed_data\", if_exists = 'replace', index = False)\n",
    "    db_conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea731e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining task operators\n",
    "\n",
    "bank_data_web_scraping = PythonOperator(task_id = \"bank_data_web_scraping\", python_callable = extract_bank_data, dag = dag)\n",
    "exchange_rate_file = PythonOperator(task_id = \"get_exchange_rate\", python_callable = extract_exchange_rate, dag = dag)\n",
    "transforming_data = PythonOperator(task_id = \"converting_currencies\", python_callable = transform, dag = dag)\n",
    "loading_to_sqlite = PythonOperator(task_id = \"loading_to_DB\", python_callable = load_to_db, dag = dag)\n",
    "\n",
    "# tasks dependencies/pipeline\n",
    "\n",
    "bank_data_web_scraping.set_downstream(transforming_data)\n",
    "exchange_rate_file.set_downstream(transforming_data)\n",
    "transforming_data.set_downstream(loading_to_sqlite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb7495",
   "metadata": {},
   "source": [
    "## Code Chanllenge API\n",
    "\n",
    "API Data Integration Code Challenge Description\n",
    "Title: API Data Handling and Integration Challenge\n",
    "\n",
    "Objective:\n",
    "Develop a Python application that fetches data from an external API, applies specified transformations, and outputs the processed data. This challenge is designed to test your abilities in API interaction, data manipulation, and the application of basic data processing principles in Python.\n",
    "\n",
    "Background:\n",
    "APIs are a crucial data source in many software systems and data pipelines. Effective handling and integration of API data are key skills for developers and data engineers, involving tasks such as data extraction, transformation, and preparation for further analysis or storage.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Extract Data:\n",
    "Write a Python function to fetch data from a given API endpoint. This function should handle network errors, API rate limits, and other common issues that can occur during API interaction.\n",
    "Transform Data:\n",
    "Implement logic to transform the raw data fetched from the API. Assume the data includes various product details; extract and format this data into a structured JSON format that focuses on specific fields like product_id, product_name, category, and price.\n",
    "Output Data:\n",
    "Instead of loading the data into a database or storage system, output the transformed data to the console or a file in a clean, readable format. This simulates the final step in an ETL process where data is made available for further use.\n",
    "Expected Deliverables:\n",
    "\n",
    "A Python script that efficiently and correctly extracts, transforms, and outputs data as described.\n",
    "Effective use of exception handling to manage potential errors during the API request.\n",
    "Logging throughout the process to track operations and facilitate debugging and monitoring.\n",
    "Evaluation Criteria:\n",
    "\n",
    "Correctness and Completeness: The script should correctly fetch and process the API data according to the specifications provided.\n",
    "Error Handling: Robust handling of errors and exceptional conditions in the API interaction.\n",
    "Code Quality: The code should be clean, well-organized, commented, and follow best practices for Python development.\n",
    "Output Formatting: The transformed data should be outputted in a structured and readable format, demonstrating an understanding of data presentation.\n",
    "Instructions for Setup and Execution:\n",
    "\n",
    "Ensure the requests and logging libraries are installed in your Python environment.\n",
    "Use any public API endpoint of your preference if tech_company related would be better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf313080",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests pandas bs4 numpy datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e4fdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def url_request(url: str):\n",
    "    \"\"\"\n",
    "    This function will do a request to the given url and return a response object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "    except requests.exceptions.ConnectionError as error:\n",
    "        print(f'Connection error ocurred. Check internet connection. Error: {error}')\n",
    "    except requests.exceptions.Timeout as error:\n",
    "        print(f'The request reached timeout. Error: {error}')\n",
    "    except requests.exceptions.HTTPError as error:\n",
    "        print(f'An HTTP error ocurred. Error: {error}')\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print(f'An error ocurred. Error: {error}')\n",
    "    return request\n",
    "\n",
    "def web_scraping(request):\n",
    "    \"\"\"\n",
    "    This function receives a response object and parses the text on it to gather specific data\n",
    "    Returns the berry url template and a list of tupples with attribute names and descriptions\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(request.text, features = \"html.parser\")\n",
    "    list_url_templates = []\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for paragraph in paragraphs:\n",
    "        if \"GET https://\" in paragraph.text:\n",
    "            pattern = r\" .*\"\n",
    "            url_template = re.search(pattern, paragraph.text)[0].strip()\n",
    "            list_url_templates.append(url_template)\n",
    "            if \"/berry/{\" in url_template:\n",
    "                pattern = r\"\\{[\\w ]+\\}\\/\"\n",
    "                berry_url_template = re.sub(pattern, r\"\", url_template)\n",
    "    tables = soup.find_all('tbody')\n",
    "    rows = tables[2].find_all('tr')\n",
    "    list_name_desc = [(row.find_all('td')[0].text.strip(), row.find_all('td')[1].text.strip()) for row in rows]\n",
    "    return (berry_url_template, list_name_desc)\n",
    "\n",
    "def extracting_json(url, headers):\n",
    "    \"\"\"\n",
    "    This function receives an url template that serves as base to construct the complete url\n",
    "    for each existing berry on the pokeAPI. Then gets all the json data as a string and casts\n",
    "    it to pyhton dict. Then creates each berry data dict is stored in a list to later populate\n",
    "    a dataframe which is returned.\n",
    "    \"\"\"\n",
    "    berry_id = 1\n",
    "    json_request = url_request(f'{url}{berry_id}')\n",
    "    code = json_request.status_code\n",
    "    berry_dict = {}\n",
    "    list_berries = []\n",
    "    while code == 200:\n",
    "        json_request = url_request(f'{url}{berry_id}')\n",
    "        berry_data = json.loads(json_request.text)\n",
    "        for header in headers:\n",
    "            key = header[0]\n",
    "            if key in [\"firmness\", \"item\", \"natural_gift_type\"]:\n",
    "                berry_dict[key] = berry_data[key][\"name\"]\n",
    "            elif key == \"flavors\":\n",
    "                berry_dict[key] = str(berry_data[key])\n",
    "            else:\n",
    "                berry_dict[key] = berry_data[key]\n",
    "        list_berries.append(berry_dict.copy())\n",
    "        berry_id += 1\n",
    "        json_request = url_request(f'{url}{berry_id}')\n",
    "        code = json_request.status_code\n",
    "    berry_df = pd.DataFrame.from_dict(list_berries)\n",
    "    return berry_df\n",
    "\n",
    "def transforming_data(dataframe):\n",
    "    \"\"\"\n",
    "    This function receives a dataframe in order to calculate the required statistics.\n",
    "    Then a new dataframe is generated and returned for the new calculations\n",
    "    \"\"\"\n",
    "\n",
    "    headers = [\"berries_names\",\n",
    "               \"growth_times\",\n",
    "               \"min_growth_time\",\n",
    "               \"median_growth_time\",\n",
    "               \"max_growth_time\",\n",
    "               \"variance_growth_time\",\n",
    "               \"mean_growth_time\",\n",
    "               \"frequency_growth_time\"]\n",
    "    \n",
    "    growth_time_list = dataframe[\"growth_time\"].to_list()\n",
    "    berry_names = dataframe[\"name\"].to_list()\n",
    "    min_growth_time = np.min(growth_time_list)\n",
    "    median_growth_time = np.median(growth_time_list)\n",
    "    max_growth_time = np.max(growth_time_list)\n",
    "    variance_growth_time = np.var(growth_time_list)\n",
    "    mean_growth_time = np.mean(growth_time_list)\n",
    "\n",
    "    frequency_growth_time = {}\n",
    "    for growth_time in growth_time_list:\n",
    "        frequency_growth_time[growth_time] = growth_time_list.count(growth_time)\n",
    "\n",
    "    calculations = [berry_names,\n",
    "                    growth_time_list,\n",
    "                    min_growth_time,\n",
    "                    median_growth_time, max_growth_time,\n",
    "                    variance_growth_time, mean_growth_time,\n",
    "                    frequency_growth_time]\n",
    "    \n",
    "    records = dict(zip(headers, calculations))\n",
    "    transformed_df = pd.DataFrame.from_dict([records])\n",
    "    return transformed_df\n",
    "\n",
    "def load_to_csv(dataframe, csv_file):\n",
    "    \"\"\"\n",
    "    This function receives a dataframe and generates a CSV file from it for reporting purposes.\n",
    "    \"\"\"\n",
    "    dataframe.to_csv(csv_file, index = False)\n",
    "    return dataframe\n",
    "\n",
    "def log_progress(message: str, log_file: str):\n",
    "    \"\"\"\n",
    "    This function generates a txt faile that contains records with a timestamp for each stage of the process\n",
    "    \"\"\"\n",
    "    print(message)\n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second \n",
    "    current_time = datetime.now() # get current timestamp \n",
    "    timestamp = current_time.strftime(timestamp_format)\n",
    "    with open(log_file, \"a\") as logging_file:\n",
    "        logging_file.write(timestamp + ',' + message + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1fed97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting data to pokeAPI\n",
      "Performing some web scraping to gather data...\n",
      "Extracting juice from berries...\n",
      "Transforming berries data\n",
      "Generating CSV file\n",
      "    id    name  growth_time  max_harvest  natural_gift_power  size  \\\n",
      "0    1   cheri            3            5                  60    20   \n",
      "1    2  chesto            3            5                  60    80   \n",
      "2    3   pecha            3            5                  60    40   \n",
      "3    4   rawst            3            5                  60    32   \n",
      "4    5  aspear            3            5                  60    50   \n",
      "..  ..     ...          ...          ...                 ...   ...   \n",
      "59  60  enigma           24            5                  80   155   \n",
      "60  61   micle           24            5                  80    41   \n",
      "61  62  custap           24            5                  80   267   \n",
      "62  63  jaboca           24            5                  80    33   \n",
      "63  64   rowap           24            5                  80    52   \n",
      "\n",
      "    smoothness  soil_dryness    firmness  \\\n",
      "0           25            15        soft   \n",
      "1           25            15  super-hard   \n",
      "2           25            15   very-soft   \n",
      "3           25            15        hard   \n",
      "4           25            15  super-hard   \n",
      "..         ...           ...         ...   \n",
      "59          60             7        hard   \n",
      "60          60             7        soft   \n",
      "61          60             7  super-hard   \n",
      "62          60             7        soft   \n",
      "63          60             7   very-soft   \n",
      "\n",
      "                                              flavors          item  \\\n",
      "0   [{'flavor': {'name': 'spicy', 'url': 'https://...   cheri-berry   \n",
      "1   [{'flavor': {'name': 'spicy', 'url': 'https://...  chesto-berry   \n",
      "2   [{'flavor': {'name': 'spicy', 'url': 'https://...   pecha-berry   \n",
      "3   [{'flavor': {'name': 'spicy', 'url': 'https://...   rawst-berry   \n",
      "4   [{'flavor': {'name': 'spicy', 'url': 'https://...  aspear-berry   \n",
      "..                                                ...           ...   \n",
      "59  [{'flavor': {'name': 'spicy', 'url': 'https://...  enigma-berry   \n",
      "60  [{'flavor': {'name': 'spicy', 'url': 'https://...   micle-berry   \n",
      "61  [{'flavor': {'name': 'spicy', 'url': 'https://...  custap-berry   \n",
      "62  [{'flavor': {'name': 'spicy', 'url': 'https://...  jaboca-berry   \n",
      "63  [{'flavor': {'name': 'spicy', 'url': 'https://...   rowap-berry   \n",
      "\n",
      "   natural_gift_type  \n",
      "0               fire  \n",
      "1              water  \n",
      "2           electric  \n",
      "3              grass  \n",
      "4                ice  \n",
      "..               ...  \n",
      "59               bug  \n",
      "60              rock  \n",
      "61             ghost  \n",
      "62            dragon  \n",
      "63              dark  \n",
      "\n",
      "[64 rows x 12 columns]\n",
      "Ploting Histogram\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poke_url = \"https://pokeapi.co/docs/v2#berries-section\"\n",
    "stats_output_file = \"poke-API_statistics.csv\"\n",
    "raw_output_file = \"all_berries_data.csv\"\n",
    "log_file = \"log_file.txt\"\n",
    "\n",
    "log_progress(\"Requesting data to pokeAPI\", log_file)\n",
    "api_request = url_request(poke_url)\n",
    "\n",
    "log_progress(\"Performing some web scraping to gather data...\", log_file)\n",
    "berry_url_template = web_scraping(api_request)[0]\n",
    "headers = web_scraping(api_request)[1]\n",
    "\n",
    "log_progress(\"Extracting juice from berries...\", log_file)\n",
    "berry_df = extracting_json(berry_url_template, headers)\n",
    "\n",
    "log_progress(\"Transforming berries data\", log_file)\n",
    "transfromed_berry_df = transforming_data(berry_df)\n",
    "\n",
    "log_progress(\"Generating CSV file\", log_file)\n",
    "load_to_csv(transfromed_berry_df, stats_output_file)\n",
    "print(load_to_csv(berry_df, raw_output_file))\n",
    "\n",
    "log_progress(\"Ploting Histogram\", log_file)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(transfromed_berry_df['frequency_growth_time'].values[0].keys(), weights=transfromed_berry_df['frequency_growth_time'].values[0].values(), bins=len(transfromed_berry_df['frequency_growth_time'].values[0]))\n",
    "plt.title('Frequency of Growth Time')\n",
    "plt.xlabel('Growth Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
